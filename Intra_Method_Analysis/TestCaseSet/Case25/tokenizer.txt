Tokenizer/tokenize:
tensorflow.device

Tokenizer/_tokenize:
tensorflow.compat.as_text

Tokenizer/detokenize:
tensorflow.device

Tokenizer/_detokenize:
tensorflow.RaggedTensor
tensorflow.is_tensor
tensorflow.RaggedTensor.from_tensor
tensorflow.compat.as_text

Tokenizer/_tokenize_tensor:
tensorflow.py_function$tensorflow.string
tensorflow.Tensor.set_shape
tensorflow.py_function$tensorflow.string$tensorflow.int32
tensorflow.RaggedTensor.from_row_lengths

Tokenizer/_python_wrapper:
tensorflow.compat.as_text
tensorflow.constant$dtype=tensorflow.string

Tokenizer/_python_wrapper_batch:
tensorflow.compat.as_text
tensorflow.nest.flatten
tensorflow.constant$dtype=tensorflow.string
tensorflow.constant$dtype=tensorflow.int32

Tokenizer/_detokenize_tensor:
tensorflow.py_function$tensorflow.string
tensorflow.Tensor.set_shape
tensorflow.map_fn$dtype=tensorflow.string

make_tokenizer:
tensorflow.io.gfile.exists
tensorflow.io.gfile.GFile

_process_stream_as_dataset:
tensorflow.string
tensorflow.TensorShape
tensorflow.data.Dataset.from_generator
tensorflow.data.Dataset.batch
tensorflow.data.Dataset.map
tensorflow.TensorSpec$dtype=tensorflow.string
tensorflow.data.Dataset.as_numpy_iterator

TensorFlowTokenizer/_map_func:
tensorflow.strings.strip
tensorflow.strings.reduce_join

TensorFlowTokenizer/_map_func:
tensorflow.strings.strip
tensorflow.strings.split

TensorFlowTokenizer/_tokenize_string:
tensorflow.constant$dtype=tensorflow.string

TensorFlowTokenizer/_detokenize_string:
tensorflow.constant$dtype=tensorflow.string

SpaceTokenizer/_tokenize_tensor:
tensorflow.strings.split

TensorFlowTokenizer/_detokenize_tensor:
tensorflow.strings.reduce_join

CharacterTokenizer/_tokenize_tensor:
tensorflow.strings.regex_replace
tensorflow.strings.unicode_split

CharacterTokenizer/_detokenize_tensor:
tensorflow.strings.reduce_join
tensorflow.strings.regex_replace
