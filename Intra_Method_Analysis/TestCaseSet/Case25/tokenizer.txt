Tokenizer/tokenize:
tensorflow.device

Tokenizer/_tokenize:
tensorflow.compat.as_text

Tokenizer/detokenize:
tensorflow.device

Tokenizer/_detokenize:
tensorflow.is_tensor
tensorflow.RaggedTensor.from_tensor
tensorflow.compat.as_text

Tokenizer/_tokenize_tensor:
tensorflow.py_function
tensorflow.Tensor.set_shape
tensorflow.py_function
tensorflow.RaggedTensor.from_row_lengths

Tokenizer/_python_wrapper:
tensorflow.compat.as_text
tensorflow.constant

Tokenizer/_python_wrapper_batch:
tensorflow.nest.flatten
tensorflow.constant
tensorflow.constant

Tokenizer/_detokenize_tensor:
tensorflow.py_function
tensorflow.Tensor.set_shape
tensorflow.map_fn

make_tokenizer:
tensorflow.io.gfile.exists
tensorflow.io.gfile.GFile

_process_stream_as_dataset:
tensorflow.TensorShape
tensorflow.data.Dataset.from_generator
tensorflow.data.Dataset.batch
tensorflow.data.Dataset.map
tensorflow.TensorSpec
tensorflow.data.Dataset.as_numpy_iterator

TensorFlowTokenizer/_map_func:
tensorflow.strings.strip
tensorflow.strings.reduce_join

TensorFlowTokenizer/_map_func:
tensorflow.strings.strip
tensorflow.strings.split

TensorFlowTokenizer/_detokenize_string:
tensorflow.constant

SpaceTokenizer/_tokenize_tensor:
tensorflow.strings.split

TensorFlowTokenizer/_detokenize_tensor:
tensorflow.strings.reduce_join

CharacterTokenizer/_tokenize_tensor:
tensorflow.strings.regex_replace
tensorflow.strings.unicode_split

CharacterTokenizer/_detokenize_tensor:
tensorflow.strings.reduce_join
tensorflow.strings.regex_replace
